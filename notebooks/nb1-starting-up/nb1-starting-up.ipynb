{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 注意事項:\n",
    "# 當您執行本Notebook的程式後，如果你要關閉Notebook，\n",
    "# 請選擇選單: File > Close and Halt 才能確實停止目前正在執行的程式，並且釋放資源\n",
    "# 如果您沒有使用以上方法，只關閉此分頁，程式仍在執行，未釋放資源，當您開啟並執行其他的Notebook時，可能會發生錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 現在可以在Cell裡，按下 按下 <Shift> + <Enter> 執行Cell裡的程式碼\n",
    "# ps. <Enter> 是換到Cell裡的下一行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Start with SparkR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkR是一個R的套件，讓使用者可以透過R的介面與習性來使用Spark。為了讓R的使用者更容易使用Spark，在Spark 2.1.1的版本中，SparkR提供了SparkDataFrame的概念，和原生R語言提供的dataframe類似，同樣提供了selection, filtering, aggregation等功能，但是是專門設計來處理大量的資料，使用者不用擔心資料太大造成的問題。透過Spark的MLlib，也可以利用SparkR進行時下最火紅的機器學行。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一個熱身的Notebook裡，我們會進行下面的操作，其中部份資料整理自[**官方網站**](http://spark.apache.org/docs/latest/sparkr.html)\n",
    "及 [**Introduction to Apache Spark with R by J. A. Dianes**](https://github.com/jadianes/spark-r-notebooks)\n",
    "\n",
    "1. 什麼是SparkDataFrame\n",
    "2. 啟動 SparkR cluster\n",
    "3. 建立SparkDataFrame\n",
    "4. SparkDataFrame基本操作 \n",
    "5. 透過SparkR執行SQL查詢\n",
    "6. 使用 ggplot2 畫圖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什麼是SparkDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkDataFrame 的概念和關聯式資料庫的表還有R語言裡的data frame一樣，是一種二維具有欄位名的資料結構，不同的是，SparkDataFrame是透過分散式的技術，將資料分散存放在不同的主機上，因此可以存放以及處理的資料，而且提供豐富的資料處理函式。\n",
    "建立SparkDataFrame時，可以透過結構化資料檔案(例如 csv檔)、外部資料庫、或由R原生的data frame轉化而得。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 啟動 SparkR cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認Linux有SPARK_HOME環境變數，才能成功載入SparkR library。\n",
    "在本次課程的虛擬機器裡環境都已設定完成，不用擔心這些細結。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步是透過 `sparkR.session()` 建立spark session。因為本課程是使用單機模式的spark，所以參數要使用`master='local'`，若是使用cluster模式的spark，要改成相對應的master位址設定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkR.session(master = \"local[*]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦成功啟動spark session後，就可以建立SparkDataFrame來操作資料。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 建立SparkDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個Notebook要讀取的檔案位於 `/home/spark/pu_workshop/data/2012-acs/ss13husa.csv`，因此先將檔案路徑在存到變數 `housing_a_file_path`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最常用來產生 SparkDataFrame 的方法是透過`read.df()`。只要在 `read.df()` 指定檔案路徑與檔案格式，可以將JSON、CSV以及Parquet檔案格式的資料轉成SparkDataFrame。除了路徑與格式外，若是CSV檔，還需要指令是否包含header列(`header`)，以及是否要由SparkR推論資料型態(`inferSchema`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_a_file_path <- file.path('', 'home','spark','pu_workshop','data','2013-acs','ss13husa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_a_df <- read.df(\n",
    "    housing_a_file_path, \n",
    "    header='true', \n",
    "    source = \"com.databricks.spark.csv\", \n",
    "    inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列出housing_a_df的Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "printSchema(housing_a_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列出 housing_a_df 的前5筆資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(housing_a_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SparkDataFrame基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkDataFrame針對結構化的資料提供很多運算，這邊介紹幾個基本的用法，詳細的[API](http://spark.apache.org/docs/latest/api/R/index.html)請自行參考線上文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting rows, columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "法1: 透過 `$` 選出 \"REGION\" 欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(select(housing_a_df, housing_a_df$REGION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "法2: 將REGION欄位名稱當成`select()`的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(select(housing_a_df, \"REGION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以用`filter()`進行篩選，將REGION為1和2的挑出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(filter(housing_a_df, housing_a_df$REGION < 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping, Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkDataFrame也提供groupby功能進行聚合運算。可以用 `n` 運算子將相同REGION的數量統計出來。\n",
    "\n",
    "[summarize用法](http://spark.apache.org/docs/latest/api/R/summarize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(summarize(groupBy(housing_a_df, housing_a_df$REGION), count = n(housing_a_df$REGION)),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以將SaprkDataFrame裡的資料進行排序。\n",
    "\n",
    "[arrange用法](http://spark.apache.org/docs/latest/api/R/arrange.html)\n",
    "[first用法](http://spark.apache.org/docs/latest/api/R/first.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REGION_counts <- summarize(groupBy(housing_a_df, housing_a_df$REGION), count = n(housing_a_df$REGION))\n",
    "first(arrange(REGION_counts, desc(REGION_counts$count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 透過SparkR執行SQL查詢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了可以透過SparkDataFrame的方法操作資料外，可以將SparkDataFrame註冊成暫時的View。就可以使用SQL的語法進行資料處理。\n",
    "\n",
    "首先，要將`housing_a_df` SparkDataFrame註冊成`housing` view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createOrReplaceTempView(housing_a_df, \"housing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的`summarize(groupBy(housing_a_df, housing_a_df$REGION), count = n(housing_a_df$REGION))` 和 `SELECT REGION , count(*) as count  FROM housing group by REGION` 是做相同的事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "region_count <- sql(\"SELECT REGION , count(*) as count  FROM housing group by REGION\")\n",
    "head(region_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`arrange(REGION_counts, desc(REGION_counts$count))`和 `SELECT REGION , count(*) as count  FROM housing group by REGION order by count desc` 是做相同的事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "region_count <- sql(\"SELECT REGION , count(*) as count  FROM housing group by REGION order by count desc\")\n",
    "first(region_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6. 使用 ggplot2 畫圖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入R的ggplot2畫圖套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ggplot2要使用R的data frame，而不是SparkDataFrame，因此需要先進行轉換。\n",
    "\n",
    "[collect用法](http://spark.apache.org/docs/latest/api/R/collect.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_region_df_local <- collect(select(housing_a_df,\"REGION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(housing_region_df_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(housing_region_df_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將`housing_region_df_local$REGION` 值改為Northeast,Midwest,South,West"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_region_df_local$REGION <- factor(\n",
    "    x=housing_region_df_local$REGION, \n",
    "    levels=c(1,2,3,4),\n",
    "    labels=c('Northeast', 'Midwest','South','West')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "進行畫圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c <- ggplot(data=housing_region_df_local, aes(x=factor(REGION)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c + geom_bar() + xlab(\"Region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
