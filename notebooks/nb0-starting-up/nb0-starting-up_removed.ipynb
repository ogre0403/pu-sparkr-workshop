{
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Downloading data and Starting with SparkR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[**Introduction to Apache Spark with R by J. A. Dianes**](https://github.com/jadianes/spark-r-notebooks)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will the 2013 American Community Survey dataset and start up a [SparkR](http://spark.apache.org/docs/latest/sparkr.html) cluster. Both are necessary steps in order to run the rest of the notebooks. After downloading the files we will have them locally and we won't need to download them again. However, we will need to init the cluster in each notebook in order to use it.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting and reading data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's first dowload the data files using R as follows."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want to persist the files, so we don't need to download them again in further notebooks.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the revious we got two zip files, `csv_pus.zip` and `csv_hus.zip`. We can now unzip them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once you unzip the contents of both files you will see up to six files. Each zip contains three files, a PDF explanatory document, and two data files in `csv` format. Each housing/population data set is divided in two pieces, \"a\" and \"b\" (where \"a\" contains states 1 to 25 and \"b\" contains states 26 to 50). Therefore:  \n",
      "\n",
      "- `ss13husa.csv`: housing data for states from 1 to 25.  \n",
      "- `ss13husb.csv`: housing data for states from 26 to 50.  \n",
      "- `ss13pusa.csv`: population data for states from 1 to 25.  \n",
      "- `ss13pusb.csv`: population data for states from 26 to 50.  \n",
      "\n",
      "We will work with these fours files in our notebooks.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Starting up a SparkR cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In further notebooks, we will explore our data by loading them into SparkSQL data frames. But first we need to init a SparkR cluster and use it to init a SparkSQL context.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing we need to do is to set up some environment variables and library paths as follows. Remember to replace the value assigned to `SPARK_HOME` with your Spark home folder.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can load the `SparkR` library as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sparkR.session(master = \"local[*]\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we can initialise the Spark context as [in the official documentation](http://spark.apache.org/docs/latest/sparkr.html#starting-up-sparkcontext-sqlcontext). In our case we are use a standalone Spark cluster with one master and seven workers. If you are running Spark in local node, use just `master='local'`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sparkR.session()\n",
      "\n",
      "#sc <- sparkR.init(master='spark://169.254.206.2:7077')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And finally we can start the SparkSQL context as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sqlContext <- sparkRSQL.init(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that's it. Once we get to this poing, we are ready to load data into SparkSQL data frames. We will do this in the next notebook."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}