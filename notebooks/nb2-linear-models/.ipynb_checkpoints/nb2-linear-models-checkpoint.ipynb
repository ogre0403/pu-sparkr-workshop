{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 SparkR提供的線性模型 (Linear Model)進行迴歸分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由前面的範列，可以看到SparkR可以用R語言的特性，使用Spark分散運算的好處。\n",
    "這個notebook會使用SparkR所提供的機器學習功能，[2013 American Community Survey](http://www.census.gov/programs-surveys/acs/data/summary-file.html)資料集建立線性模型，並用這個模型來預測資料，並檢測模型的正確性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 啟動Spark Session & 建立 SparkDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和之前一樣，要先建立Spark Session。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkR.session(master = \"local[*]\" , sparkConfig = list(spark.driver.extraJavaOptions = \"-Xss10m\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仿照之前的作法，載入`ss13husa.csv`和`ss13husb.csv`二個檔案，並建立SparkDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_a_file_path <- file.path('', 'home','spark','pu_workshop','data','2013-acs','ss13husa.csv')\n",
    "housing_b_file_path <- file.path('', 'home','spark','pu_workshop','data','2013-acs','ss13husb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_a_df <- read.df(\n",
    "    housing_a_file_path, \n",
    "    header='true', \n",
    "    source = \"com.databricks.spark.csv\", \n",
    "    inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_b_df <- read.df(\n",
    "    housing_b_file_path, \n",
    "    header='true', \n",
    "    source = \"com.databricks.spark.csv\", \n",
    "    inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "透過csv，產生二個SparkDataFrame，要先將這二個SparkDataFrame結合起來。\n",
    "\n",
    "[rbind](http://spark.apache.org/docs/latest/api/R/rbind.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_df <- rbind(housing_a_df, housing_b_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rbind就是把二個SparkDataFrame做聯集，因此，總資料筆數應該就是二個SparkDataFrame的加總。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrow_a <- nrow(housing_a_df)\n",
    "nrow_b <- nrow(housing_b_df)\n",
    "nrows <- nrow(housing_df)\n",
    "nrow_a\n",
    "nrow_b\n",
    "nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(housing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 清洗資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在將資料由csv讀成SparkDataFrame時，使用inferSchema，所以數值欄位的資料型態會被定義為int。但是在REGION欄位，這些數字只是用來代表是某一個種類，並不是有意義的數值，因此可用[cast](http://spark.apache.org/docs/latest/api/R/cast.html)要由數值資料(numeric variable)轉換成類別資料(categorical variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_df$ST <- cast(housing_df$ST, \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_df$REGION <- cast(housing_df$REGION, \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料集中有些資料的欄位是null，在後續就沒辦法做迴歸，因此，要先將有問題的資料去除。這裡是用[filter](http://spark.apache.org/docs/latest/api/R/filter.html)，過濾值是null的資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_with_valp_df <- filter(\n",
    "    housing_df, \n",
    "    isNotNull(housing_df$VALP) \n",
    "    & isNotNull(housing_df$TAXP)\n",
    "    & isNotNull(housing_df$INSP)\n",
    "    & isNotNull(housing_df$ACR)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "檢查合法的資料剩多少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrows <- nrow(housing_with_valp_df)\n",
    "nrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 建立訓練 & 測試 資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`randomSplit` 將資料集分成二部份，一部份用來建立模型，一部份用來驗證模型正確性。建模與測試的資料集由原始資料依比例9:1取出。\n",
    "\n",
    "[randomSplit](http://spark.apache.org/docs/latest/api/R/randomSplit.html)。**`randomSplit`是Spark 2.0.0後導入的函數，好像還有bug，若無法執行請多執行幾次。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_df_list <- randomSplit(housing_with_valp_df, c(1,9), 0)\n",
    "housing_df_test <- housing_df_list[[1]]\n",
    "housing_df_train <- housing_df_list[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrow(housing_df_test)\n",
    "nrow(housing_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 訓練線性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們使用Spark提供的[spark.glm](http://spark.apache.org/docs/latest/api/R/spark.glm.html)來建立線性模型。\n",
    "和[R語言的glm](http://www.statmethods.net/advstats/glm.html)非常類似，用法幾乎完全一樣。\n",
    "\n",
    "不同的是，`spark.glm`是針對SparkDataFrame做運算，而不是針對R的data frame做運算，因此在遇到大量資料的時候，才能進行平行處理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在`glm`裡，我們目的是要預測房地產價值。  \n",
    "- `VALP` or Property value.\n",
    "\n",
    "而在`glm`用來預測的變數有\n",
    "- `RMSP` or number of rooms.\n",
    "- `ACR` the lot size.\n",
    "- `INSP` or insurance cost.\n",
    "- `TAXP` or taxes cost.\n",
    "- `ELEP` or electricity cost.\n",
    "- `GASP` or gas cost.\n",
    "- `ST` that is the state code.\n",
    "- `REGION` that identifies the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model <- spark.glm(\n",
    "    VALP ~ RMSP + ACR + INSP + TAXP + ELEP + GASP + ST + REGION, \n",
    "    data = housing_df_train, \n",
    "    family = \"gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "透過[summay](http://spark.apache.org/docs/latest/api/R/summary.html)，可以知道線性模型裡每個變數的係數，可以看出不同變數的影響程度。\n",
    "\n",
    "搭配[data dictionary](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt)，我們可以發覺加州(`ST_6`)和夏威夷(`ST_15`)的房地產有正影響。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 使用測試資料驗證模型準確度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用測試資料進行預測。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions <- predict(model, newData = housing_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下預測的結果和實際值是否相符或相差很多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(select(predictions, \n",
    "    \"prediction\",\"VALP\",\"RMSP\",\"ACR\",\"INSP\",\"TAXP\",\"ELEP\",\"GASP\",\"ST\",\"REGION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以計算[R2](https://en.wikipedia.org/wiki/Coefficient_of_determination)來判斷模型的好壞，R2=1，代表模型完全符合資料，所以R2值越大越好。\n",
    "\n",
    "$S_{res} = \\sum_i (VALP_i-prediction_i)^2$\n",
    "\n",
    "$S_{tot} = \\sum_i (VALP_i - \\overline{VALP})^2$\n",
    "\n",
    "$ R^2 = 1 - \\frac{S_{res}}{S_{tot}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALP_mean <- collect(agg(\n",
    "    housing_df_train, \n",
    "    AVG_VALP=mean(housing_df_train$VALP)\n",
    "))$AVG_VALP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALP_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions <- transform(\n",
    "    predictions, \n",
    "    S_res=(predictions$VALP - predictions$prediction)**2, \n",
    "    S_tot=(predictions$VALP - VALP_mean)**2)\n",
    "head(select(predictions, \"VALP\", \"prediction\", \"S_res\", \"S_tot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows_test <- nrow(housing_df_test)\n",
    "residuals <- collect(agg(\n",
    "    predictions, \n",
    "    SS_res=sum(predictions$S_res),\n",
    "    SS_tot=sum(predictions$S_tot)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R2 <- 1.0 - (residuals$SS_res/residuals$SS_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of 0.40 doesn't speak very well about our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
