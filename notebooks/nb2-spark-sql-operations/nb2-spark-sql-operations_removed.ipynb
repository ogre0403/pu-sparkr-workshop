{
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data frame operations with SparkSQL and SparkR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[**Introduction to Apache Spark with R by J. A. Dianes**](https://github.com/jadianes/spark-r-notebooks)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will introduce more advanced concepts about SparkSQL with R that you can find in the [SparkR documentation](http://spark.apache.org/docs/latest/sparkr.html), applied to the [2013 American Community Survey](http://www.census.gov/programs-surveys/acs/data/summary-file.html) housing data. These concepts are related with data frame manipulation, including data slicing, summary statistics, and aggregations. We will use them in combination with [ggplot2](http://ggplot2.org/) visualisations. We will explain what we do at every step but, if you want to go deeper into `ggplot2` for exploratory data analysis, I did this [Udacity on-line course](https://www.udacity.com/course/data-analysis-with-r--ud651) in the past and I highly recommend it!  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating a SparkSQL context and loading data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to explore our data, we first need to load it into a SparkSQL data frame. But first we need to init a SparkSQL context. The first thing we need to do is to set up some environment variables and library paths as follows. Remember to replace the value assigned to `SPARK_HOME` with your Spark home folder.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Spark home and R libs\n",
      "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can load the `SparkR` library as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqlContext <- sparkR.session(master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"2g\") ,sparkPackages = \"com.databricks:spark-csv_2.11:1.2.0\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we can initialise the Spark context as [in the official documentation](http://spark.apache.org/docs/latest/sparkr.html#starting-up-sparkcontext-sqlcontext). In our case we are use a standalone Spark cluster with one master and seven workers. If you are running Spark in local node, use just `master='local'`. Additionally, we require a Spark package from Databricks to read CSV files (more on this in the [previous notebook](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb1-spark-sql-basics/nb1-spark-sql-basics.ipynb)). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sc <- sparkR.init(master='spark://169.254.206.2:7077', sparkPackages=\"com.databricks:spark-csv_2.11:1.2.0\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And finally we can start the SparkSQL context as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# sqlContext <- sparkRSQL.init(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our SparkSQL context ready, we can use it to load our CSV data into data frames. We have downloaded our [2013 American Community Survey dataset](http://www.census.gov/programs-surveys/acs/data/summary-file.html) files in [notebook 0](https://github.com/jadianes/spark-r-notebooks/tree/master/notebooks/nb0-starting-up/nb0-starting-up.ipynb), so they should be stored locally. Remember to set the right path for your data files in the first line, ours is `/nfs/data/2013-acs/ss13husa.csv`.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_a_file_path <- file.path('', 'home','spark','pu_workshop','data','2013-acs','ss13husa.csv')\n",
      "housing_b_file_path <- file.path('', 'home','spark','pu_workshop','data','2013-acs','ss13husa.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's read into a SparkSQL dataframe. We need to pass four parameters in addition to the `sqlContext`:  \n",
      "\n",
      "- The file path.  \n",
      "- `header='true'` since our `csv` files have a header with the column names. \n",
      "- Indicate that we want the library to infer the schema.  \n",
      "- And the source type (the Databricks package in this case). \n",
      "\n",
      "And we have two separate files for both, housing and population data. We need to join them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_a_df <- read.df(\n",
      "    #sqlContext, \n",
      "                        housing_a_file_path, \n",
      "                        header='true', \n",
      "                        source = \"com.databricks.spark.csv\", \n",
      "                        inferSchema='true')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_b_df <- read.df(\n",
      "    #sqlContext, \n",
      "                        housing_b_file_path, \n",
      "                        header='true', \n",
      "                        source = \"com.databricks.spark.csv\", \n",
      "                        inferSchema='true')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_df <- rbind(housing_a_df, housing_b_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check that we have everything there by counting the files and listing a few of them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nrow(housing_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "head(housing_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Giving ggplot2 a try"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we dive into data selection and aggregations, let's try plotting something using [ggplot2](http://ggplot2.org). We will use this library all the time during our exploratory data analysis, and we better mke sure how to use it with SparkSQL results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# if it isn't installed, \n",
      "# install install.packages(\"ggplot2\") \n",
      "# from the R console, specifying a CRAN mirror\n",
      "\n",
      "library(ggplot2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What if we directly try to use our SparkSQL `DataFrame` class into a `ggplot`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c <- ggplot(data=housing_df, aes(x=factor(REGION)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obviously it doesn't work that way. The `ggplot` function doesn't know how to deal with that type of distributed data frames (the Spark ones). Instead, we need to collect the data locally as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_region_df_local <- collect(select(housing_df,\"REGION\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have a look at what we got."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "str(housing_region_df_local)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That is, when we collect results from a SparkSQL `DataFrame` we get a regular R `data.frame`. Very convenient since we can manipulate it as we need to. For example, let's convert that `int` values we have for `REGION` to a factor with the proper names. From our [data dictionary](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt) we will get the meaning of the `REGION` variable, as well as the different values it can take."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_region_df_local$REGION <- factor(\n",
      "    x=housing_region_df_local$REGION, \n",
      "    levels=c(1,2,3,4,9),\n",
      "    labels=c('Northeast', 'Midwest','South','West','Puerto Rico')\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we are ready to create the ggplot object as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "c <- ggplot(data=housing_region_df_local, aes(x=factor(REGION)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we can give the plot a proper representation (e.g. a [bar plot](http://docs.ggplot2.org/current/geom_bar.html)). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c + geom_bar() + xlab(\"Region\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will always follow the same approach. First we make some sort of oepration with the SparkSQL `DataFrame` object (e.g. a selection), then we collect results, and then we prepare the resulting `data.frame` to be represented using ggplot2. But think about the previous. We just represented all the samples for a given column. That is almost a million and a half data points, and we are pushing our local R environment and ggplot2 a lot. In the case of the bar plot we didn't really experience any problems, cause is sort of aggregating data inside. But we will struggle to do scatter plots this way. The preferred kind of visualisations will be those that come from data from aggreations on SparkSQL `DataFrames` as we will see in further sections."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section we will demonstrate how to select data from a SparkSQL `DataFrame` object using SparkR. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "`select` and `selectExpr`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We already made use of the `select` function. but let's have a look at the documentation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "?select"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have two flavours of `select`. One that gets a list of column names (this is the one we used so far) and another one called `selectExpr` that we pass a string containing a SQL expression. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course we can pass more than a column name."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collect(select(housing_df, \"REGION\", \"VALP\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When passing column names we can also use the R notation `data.frame$column.name` so familiar to R users. For example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collect(select(housing_df, housing_df$VALP))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How does this notation compares to the name-based one in terms of performance?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    collect(select(housing_df, housing_df$VALP))\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    collect(select(housing_df, \"VALP\"))\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When using the $ notation, we can even pass expressions as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "head(select(housing_df, housing_df$VALP / 100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So what's the point of `selectExpr` then? Well, we can pass more complex SQL expressions. For example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "head(selectExpr(housing_df, \"(VALP / 100) as VALP_by_100\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "`filter`, `subset`, and `sql`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The previous functions allow us selecting columns. In order to select rows, we will use `filter` and `contains`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "?filter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With `filter` we filter the rows of a DataFrame according to a given condition that we pass as argument. We can define conditions as SQL conditions using column names or by using the $ notation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For example, and following with our property values column, let's select property values higher than 1000 for the south region."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    housing_valp_1000 <- collect(filter(select(housing_df, \"REGION\", \"VALP\"), \"VALP > 1000\"))\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_1000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Take into account that we can also perform the previous selection and filtering by using SQL queries agains the SparkSQL `DataFrame`. In order to do that we need to register the table as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "registerTempTable(housing_df, \"housing\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then we can use SparkR `sql` function using the `sqlContext` as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    housing_valp_1000_sql <- collect(sql(sqlContext, \"SELECT REGION, VALP FROM housing WHERE VALP >= 1000\"))\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_1000_sql"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This last method might result more clear and flexible when we need to perform complex queries with multiple conditions. Using `filter` and `select` combinations might get verbose versus the clarity of the SQL lingua franca."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But there is another way of subsetting dataframes in a functional way. A way that is very familiar to R users. It is by using the function `subset`. Just have a look at the help page."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "?subset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And we use it as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    housing_valp_1000_subset <- collect(subset(housing_df, housing_df$VALP>1000, c(\"REGION\",\"VALP\")))\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_1000_subset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even more, we can use the `[]` notation we use with R `data.frame` objects with SparkSQL `DataFrames` thanks to SparkR. For example.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    housing_valp_1000_bracket <- collect(housing_df[housing_df$VALP>1000, c(\"REGION\",\"VALP\")])\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_1000_bracket"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That is, we have up to four different ways of subseting a data frame with SparkR. We can plot any of the previous resulting data frames with a ggplot2 chart as we did before."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_1000_bracket$REGION <- factor(\n",
      "    x=housing_valp_1000_bracket$REGION, \n",
      "    levels=c(1,2,3,4,9),\n",
      "    labels=c('Northeast', 'Midwest','South','West','Puerto Rico')\n",
      ")\n",
      "c <- ggplot(data=housing_region_df_local, aes(x=factor(REGION)))\n",
      "c + geom_bar() + ggtitle(\"Samples with VALP>1000\") + xlab(\"Region\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, a function that is useful, specially when imputing missing values in data frames is [`isNaN`](https://spark.apache.org/docs/latest/api/R/isNaN.html) that can be applied to columns as we do with regular R data frames."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data aggregation and sorting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous notebook we already had a look at `summary`/`describe` that we can pass column names and get summary statistics that way. If we want instead to be specific about the statistic we want, SparkR also defines the following aggregation functions that we can apply to `DataFrame` objects columns:  \n",
      "\n",
      "- [`avg`](https://spark.apache.org/docs/latest/api/R/avg.html)    \n",
      "- [`min`](https://spark.apache.org/docs/latest/api/R/min.html)    \n",
      "- [`max`](https://spark.apache.org/docs/latest/api/R/max.html)   \n",
      "- [`sum`](https://spark.apache.org/docs/latest/api/R/sum.html)  \n",
      "- [`countDistinct`](https://spark.apache.org/docs/latest/api/R/countDistinct.html)  \n",
      "- [`sumDistinct`](https://spark.apache.org/docs/latest/api/R/sumDistinct.html)  \n",
      "\n",
      "We use them passing columns with the $ notation, and they return columns, so they need to be part of a `select` call for a `DataFrame`. For example.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collect(select(housing_df, avg(housing_df$VALP)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "`groupBy` and `summarize`/`agg`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "A basic operation when doing data aggregations on data frames is `groupBy`. Basically it groups the `DataFrame` we pass using the specified columns, so we can run aggregation on them. We use it in combination with `summarize`/`agg` in order to apply aggregation functions. For example, using the previous `avg` example, let's averagle property values by region as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_avg_valp <- collect(agg(\n",
      "    groupBy(housing_df, \"REGION\"), \n",
      "    NUM_PROPERTIES=n(housing_df$REGION),\n",
      "    AVG_VALP = avg(housing_df$VALP), \n",
      "    MAX_VALUE=max(housing_df$VALP),\n",
      "    MIN_VALUE=min(housing_df$VALP)\n",
      "))\n",
      "housing_avg_valp$REGION <- factor(\n",
      "    housing_avg_valp$REGION, \n",
      "    levels=c(1,2,3,4,9), \n",
      "    labels=c('Northeast', 'Midwest','South','West','Puerto Rico')\n",
      ")\n",
      "housing_avg_valp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "We can add as many summary/aggregation columns as functions we want to calculate. There is also the posibility of adding several levels of grouping. For example, let's add the number of bedrooms (`BDSP` in our [dictionary](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt)) as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_avg_valp <- collect(agg(\n",
      "    groupBy(housing_df, \"REGION\", \"BDSP\"), \n",
      "    NUM_PROPERTIES=n(housing_df$REGION),\n",
      "    AVG_VALP = avg(housing_df$VALP), \n",
      "    MAX_VALUE=max(housing_df$VALP),\n",
      "    MIN_VALUE=min(housing_df$VALP)\n",
      "))\n",
      "housing_avg_valp$REGION <- factor(\n",
      "    housing_avg_valp$REGION, \n",
      "    levels=c(1,2,3,4,9), \n",
      "    labels=c('Northeast', 'Midwest','South','West','Puerto Rico')\n",
      ")\n",
      "housing_avg_valp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "`arrange`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "And finally, we can arrange a `DataFrame`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "head(arrange(select(housing_df, \"REGION\", \"VALP\"), desc(housing_df$VALP)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or our aggregations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_avg_agg <- agg(\n",
      "        groupBy(housing_df, \"REGION\", \"BDSP\"), \n",
      "        NUM_PROPERTIES=n(housing_df$REGION),\n",
      "        AVG_VALP = avg(housing_df$VALP), \n",
      "        MAX_VALUE=max(housing_df$VALP),\n",
      "        MIN_VALUE=min(housing_df$VALP)\n",
      "    )\n",
      "housing_avg_sorted <- head(arrange(housing_avg_agg, desc(housing_avg_agg$AVG_VALP)))\n",
      "\n",
      "housing_avg_sorted$REGION <- factor(\n",
      "    housing_avg_sorted$REGION, \n",
      "    levels=c(1,2,3,4,9), \n",
      "    labels=c('Northeast', 'Midwest','South','West','Puerto Rico')\n",
      ")\n",
      "housing_avg_sorted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that's it. In the next notebook we will dig deeper into property values using these operations and ggplot2 charts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}