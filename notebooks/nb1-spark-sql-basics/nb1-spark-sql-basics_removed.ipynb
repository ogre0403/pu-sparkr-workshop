{
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "SparkSQL basics with SparkR  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[**Introduction to Apache Spark with R by J. A. Dianes**](https://github.com/jadianes/spark-r-notebooks)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will introduce basic concepts about SparkSQL with R that you can find in the [SparkR documentation](http://spark.apache.org/docs/latest/sparkr.html), applied to the [2013 American Community Survey dataset](http://www.census.gov/programs-surveys/acs/data/summary-file.html). We will do two things, read data into a SparkSQL data frame, and have a quick look at the schema and what we have read. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating a SparkSQL context"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In further notebooks, we will explore our data by loading them into SparkSQL data frames. But first we need to init a SparkSQL context. The first thing we need to do is to set up some environment variables and library paths as follows. Remember to replace the value assigned to `SPARK_HOME` with your Spark home folder.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Spark home and R libs\n",
      "# Sys.setenv(SPARK_HOME='/home/cluster/spark-1.5.0-bin-hadoop2.6')\n",
      "# .libPaths(c(file.path(Sys.getenv('SPARK_HOME'), 'R', 'lib'), .libPaths()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can load the `SparkR` library as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we can initialise the Spark context as [in the official documentation](http://spark.apache.org/docs/latest/sparkr.html#starting-up-sparkcontext-sqlcontext). In our case we are use a standalone Spark cluster with one master and seven workers. If you are running Spark in local node, use just `master='local'`. Additionally, we require a Spark package from Databricks to read CSV files (more on this in the next section).  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sc <- sparkR.init(master='spark://169.254.206.2:7077', sparkPackages=\"com.databricks:spark-csv_2.11:1.2.0\")\n",
      "sqlContext <- sparkR.session(master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"2g\") ,sparkPackages = \"com.databricks:spark-csv_2.11:1.2.0\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And finally we can start the SparkSQL context as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sqlContext <- sparkRSQL.init(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating SparkSQL data frames"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Reading CSV data using Databricks csv extension"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "The easiest way to get our CSV data into a [SparkSQL dataframe](http://spark.apache.org/docs/latest/sparkr.html#creating-dataframes), is by using [Databricks CSV extension](https://github.com/databricks/spark-csv) to read SparkSQL dataframes directly from csv files. In any case, remember to set the right path for your data files in the first line, ours is `/home/spark/pu_workshop/data/2013-acs/ss13husa.csv`.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_a_file_path <- file.path('', 'home','spark','pu_workshop','data','2013-acs','ss13husa.csv')\n",
      "housing_b_file_path <- file.path('', 'home','spark','pu_workshop','data','2013-acs','ss13husa.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's read into a SparkSQL dataframe. We need to pass four parameters in addition to the `sqlContext`:  \n",
      "\n",
      "- The file path.  \n",
      "- `header='true'` since our `csv` files have a header with the column names. \n",
      "- Indicate that we want the library to infer the schema.  \n",
      "- And the source type (the Databricks package in this case).  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    housing_a_df <- read.df(\n",
      "        #sqlContext, \n",
      "                        housing_a_file_path, \n",
      "                        header='true', \n",
      "                        source = \"com.databricks.spark.csv\", \n",
      "                        inferSchema='true')\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have a look at the inferred schema."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    printSchema(housing_a_df)\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks good. Let's have a look at the first few rows.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "head(housing_a_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And let's count how many rows do we have in the first dataset. For that we will use `nrow` as we do with regular R data frames. Let's have a look at the documentation.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "?nrow"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is just a definition of `nrow` by the package *SparkR*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nrow(housing_a_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's read the second housing data frame and count the number of rows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    housing_b_df <- read.df(\n",
      "        #sqlContext, \n",
      "                        housing_b_file_path, \n",
      "                        header='true', \n",
      "                        source = \"com.databricks.spark.csv\", \n",
      "                        inferSchema='true')\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nrow(housing_b_df))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Merging data frames"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use `rbind()` as we do with regular R data frames to put both of them together. Let's have a look at the documentation.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "?rbind"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, SparkR redefines many of the common R functions to work with SparkSQL data frames. Let's actually use `rbind` as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_df <- rbind(housing_a_df, housing_b_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And let's count how many rows do we have in the complete data frame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    housing_samples <- nrow(housing_df)\n",
      ")\n",
      "print(housing_samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, let's get a feeling of what is to explore data using SparkR by using the `summary` function on the data frame. Let's first have a look at the documentation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "? summary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that we are redirected to `describe`, that optionally accepts column names. Let's use it with the whole data frame, that is, the 231 columns and 1476313 rows. *Note*: We use `collect` here because the results of `describe` are given as a `DataFrame` object and we need to print them in the notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system.time(\n",
      "    housing_summary <- describe(housing_df)\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collect(housing_summary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or we can select individual column summaries using `select` as follows (here is [a dictionary](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt) of each column meaning) where `VALP` is the property value.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collect(select(housing_summary,\"VALP\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that's it. In this notebook we have shown how to load a CSV file into an SparkSQL data frame using SparkR. We also had a look at the data loaded, mainly to the number of samples loaded, and the data summary."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}