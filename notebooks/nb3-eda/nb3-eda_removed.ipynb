{
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exploratory Data Analysis with SparkR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[**Introduction to Apache Spark with R by J. A. Dianes**](https://github.com/jadianes/spark-r-notebooks)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will use all the SparkSQL operation we learned before in order to explore property value in relation to other variables in the [2013 American Community Survey](http://www.census.gov/programs-surveys/acs/data/summary-file.html) dataset. The whole point of R on Spark is to introduce Spark scalability into R data analysis pipelines. With this idea in mind, we have seen how [SparkR documentation](http://spark.apache.org/docs/latest/sparkr.html) introduces data types and functions that are very similar to what we are used to when using regular R. We will combine these with [ggplot2](http://ggplot2.org) in order to explore relationships between our varaibles. We will explain what we do at every step but, if you want to go deeper into `ggplot2` for exploratory data analysis, I did this [Udacity on-line course](https://www.udacity.com/course/data-analysis-with-r--ud651) in the past and I highly recommend it! \n",
      "\n",
      "In this notebook we will explore three of these values:  \n",
      "- We will explore `ST` using maps, as an example of using a geographical indicator to predict property value.  \n",
      "- We will explore `ACR` as an example of categorical value, trying to compare different populations defined by the different `ACR` values.\n",
      "- We will explore `RMSP` as an example of ordinal value.\n",
      "\n",
      "In the next notebook, we will use SparkR `glm` function to build a linear model and check the significance of each of these variables as predictors for property value.\n",
      "\n",
      "So let's dive into it!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating a SparkSQL context and loading data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "In order to explore our data, we first need to load it into a SparkSQL data frame. But first we need to init a SparkSQL context. The first thing we need to do is to set up some environment variables and library paths as follows. Remember to replace the value assigned to `SPARK_HOME` with your Spark home folder.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Spark home and R libs\n",
      "library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can load the `SparkR` library as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqlContext <- sparkR.session(master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"2g\") ,sparkPackages = \"com.databricks:spark-csv_2.11:1.2.0\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we can initialise the Spark context as [in the official documentation](http://spark.apache.org/docs/latest/sparkr.html#starting-up-sparkcontext-sqlcontext). In our case we are use a standalone Spark cluster with one master and seven workers. If you are running Spark in local node, use just `master='local'`. Additionally, we require a Spark package from Databricks to read CSV files (more on this in the [previous notebook](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb1-spark-sql-basics/nb1-spark-sql-basics.ipynb)). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sc <- sparkR.init(master='spark://169.254.206.2:7077', sparkPackages=\"com.databricks:spark-csv_2.11:1.2.0\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And finally we can start the SparkSQL context as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# sqlContext <- sparkRSQL.init(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our SparkSQL context ready, we can use it to load our CSV data into data frames. We have downloaded our [2013 American Community Survey dataset](http://www.census.gov/programs-surveys/acs/data/summary-file.html) files in [notebook 0](https://github.com/jadianes/spark-r-notebooks/tree/master/notebooks/nb0-starting-up/nb0-starting-up.ipynb), so they should be stored locally. Remember to set the right path for your data files in the first line, ours is `/nfs/data/2013-acs/ss13husa.csv`.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_a_file_path <- file.path('', 'home','spark','pu_workshop','data','2013-acs','ss13husa.csv')\n",
      "housing_b_file_path <- file.path('', 'home','spark','pu_workshop','data','2013-acs','ss13husa.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's read into a SparkSQL dataframe. We need to pass four parameters in addition to the `sqlContext`:  \n",
      "\n",
      "- The file path.  \n",
      "- `header='true'` since our `csv` files have a header with the column names. \n",
      "- Indicate that we want the library to infer the schema.  \n",
      "- And the source type (the Databricks package in this case). \n",
      "\n",
      "And we have two separate files for both, housing and population data. We need to join them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_a_df <- read.df(\n",
      "    #sqlContext, \n",
      "                        housing_a_file_path, \n",
      "                        header='true', \n",
      "                        source = \"com.databricks.spark.csv\", \n",
      "                        inferSchema='true')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_b_df <- read.df(\n",
      "    #sqlContext, \n",
      "                        housing_b_file_path, \n",
      "                        header='true', \n",
      "                        source = \"com.databricks.spark.csv\", \n",
      "                        inferSchema='true')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_df <- rbind(housing_a_df, housing_b_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check that we have everything there by counting the files and listing a few of them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nrow(housing_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "head(housing_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A priori and informal variable selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From our [data dictionary](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt) we know that the variable `VALP` represents a property value. Our goal in this notebook is to explore this variable, specially what other variables might have certain relationship with it. \n",
      "\n",
      "A possible way of doing this is to calculate correlation values between `VALP` and any other variable in the 230 set of them. For this we need numeric variables, and most of our dataset features are not numeric. There are other ways of doing this for categorical values but in our case we will use a more informal approach.  \n",
      "\n",
      "First we will have a look at the dictionary and extract a list of candidates that to our knowledge could influence a propery value. Then we will use aggregations, tables, and plots, to explore relationships. The process won't be exhaustive but will show how to do this kind of analysis. Actually we won't to make this notebook so long, so we wil be fine with a reduced list of candidates. Hopefully we will get a list of variables to be used in a predictive linear model in further notebooks."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are the 16 variables we will consider when building a linear model. Please, refer to the [dictionary]() to check their possible values:  \n",
      "\n",
      "- `ST`: state code, as the main geographical variable, since it someway includes region and department.  \n",
      "- `ACR`: Lot size.\n",
      "- `BLD`: Units in structure.\n",
      "- `RMSP`: Number of Rooms.\n",
      "- `VACS`: Vacancy status.\n",
      "- `YBL`: Year first built.\n",
      "- A series of facilities variables:\n",
      "  - `ACCESS`: Access to the Internet.\n",
      "  - `BATH`: Bathtub or shower.\n",
      "  - `RWAT`: Hot and cold running water.\n",
      "  - `PLM`: Complete plumbing facilities.\n",
      "  - `TEL`: Telephone.\n",
      "  - `TOIL`: Flush toilet.\n",
      "- A series of cost-related variables:\n",
      "  - `ELEP`: Electricity (monthly cost).\n",
      "  - `FULP`: Fuel cost(yearly cost for fuels other than gas and electricity).\n",
      "  - `GASP`: Gas (monthly cost).\n",
      "  - `WATP`: Water (yearly cost)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will explore three of these values:  \n",
      "- We will explore `ST` using maps, as an example of using a geographical indicator to predict property value.  \n",
      "- We will explore `ACR` as an example of categorical value, trying to compare different populations defined by the different `ACR` values.\n",
      "- We will explore `ELEP` as an example of numerical variable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the next notebook, we will use SparkR `glm` function to build a linear model and check the significance of each of these variables as predictors for property value."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "collapsed": true
     },
     "source": [
      "Exploring property value by state using ggplot maps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To create a map of the United States with the states colored according to `VALP`, we can use ggplot2's `map_data` function. This functionality requires the [R package *maps*](https://cran.r-project.org/web/packages/maps/index.html), that you might need to install from the R shell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you might need to install maps by running from the R console:\n",
      "# install.packages(\"maps\")\n",
      "\n",
      "library(ggplot2)\n",
      "\n",
      "states_map <- map_data(\"state\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our map contains a series of geometric shapes, latitudes, codes, and region names."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "str(states_map)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `region` variable there is the state name in the case of the *state* map, corresponding to the US states. We will need to match that with our housing dataset state code some way. But let's first reduce the housing dataset to what we want to represent in the map."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataset we want to visualise is the average property value by state. We can use SparkR as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_avg_valp <- collect(\n",
      "    agg(\n",
      "        groupBy(housing_df, \"ST\"),\n",
      "        AVG_VALP=avg(housing_df$VALP)\n",
      "    )\n",
      ")\n",
      "head(housing_avg_valp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to map the `ST` column to state names, so we can associate the right value to the right poligon in the `state_map` map. We have provided a csv file containing this mapping. Let's read it into a regular R data frame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "state_names <- read.csv(\"states.csv\")\n",
      "head(state_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So all we have to do is change the code for the name. For example, using a factor variable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_avg_valp$region <- factor(housing_avg_valp$ST, levels=state_names$st, labels=tolower(state_names$name))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "head(housing_avg_valp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then we are ready to merge the dataset with the map as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "merged_data <- merge(states_map, housing_avg_valp, by=\"region\")\n",
      "head(merged_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we can use *ggplot2* with a `geom_polygon` to plot the previous merged data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ggplot(merged_data, aes(x = long, y = lat, group = group, fill = AVG_VALP)) + \n",
      "    geom_polygon(color = \"black\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hey, that property value map seems to make sense, right? States like California (West Coast), New York, or Washington DC (North East Coast) have the highest average property value, while interior states have the lowest."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have visualised the relationship between the state code and the property value. It seems to be at least categorical. We might try to define also a latitude-longitude relationship but it might be too weak to worth the effort. In any case I invite you to give it a try."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Lot size and property value"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "The variable `ACR` or lot size can take four different values, as given in the [data dictionary](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt). These are:  \n",
      "\n",
      "  - b:  N/A (GQ/not a one-family house or mobile home)\n",
      "  - 1:  House on less than one acre\n",
      "  - 2:  House on one to less than ten acres \n",
      "  - 3:  House on ten or more acres"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to examine what kind of relationship do we have between this variable and the property value one, we might be tempted to use a [ggplot2 boxplot](http://docs.ggplot2.org/0.9.3.1/geom_boxplot.html) as follows.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_acr <- collect(select(housing_df, \"ACR\", \"VALP\"))\n",
      "housing_acr$ACR <- factor(\n",
      "    housing_acr$ACR, \n",
      "    levels=c('b','1','2','3'), \n",
      "    labels=c('N/A','Less 1 acr','1-10 acr','10 or more acr')\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lot_acr <- ggplot(data=housing_acr, aes(x=ACR, y=VALP))\n",
      "lot_acr + geom_boxplot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But this is not very scalable. Actually it took quite a long time to render that plot and might not even work in some machines. What if we have a dataset not with million and a half samples but with hundreds of millions? Since we are using Spark for this exploratiory data analysis, we should leave all the computation efforts to our cluster and pass ggplot just a reduced and already digested dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start by aggregating the property value by lot size, using SparkR, as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_by_acr <- collect(agg(\n",
      "    groupBy(housing_df,\"ACR\"),\n",
      "    AVG_VALP = avg(housing_df$VALP),\n",
      "    MIN_VALP = min(housing_df$VALP),\n",
      "    MAX_VALP = max(housing_df$VALP),\n",
      "    N = count(housing_df$VALP)\n",
      "))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we can just use a table to check the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_by_acr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Median values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The min and max values don't seem to be very useful. Moreover, average values are highly affected by adge values and outliers, etc, so it is normal that we have them very similar. This might be the case in our distribution, but we better calculate the median value, that is not affected that way. There isn't a column aggregation function for the median in the current version of SparkR, so we have to calculate that ourselves. We will filter and arrange a sql query as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "createOrReplaceTempView(housing_df, \"housing\")\n",
      "housing_valp_1_acr <- collect(sql(\"SELECT ACR,VALP FROM housing WHERE ACR=1 ORDER BY VALP\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nrow(housing_valp_1_acr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we have an even number of rows, the median value will be the mean between two values around the mid point."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_1_acr_median = (housing_valp_1_acr[nrow(housing_valp_1_acr)/2,\"VALP\"] + housing_valp_1_acr[nrow(housing_valp_1_acr)/2+1,\"VALP\"] ) / 2.0\n",
      "housing_valp_1_acr_median"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's repeat the same for each of the other two ACR values we have meaningful value s(we will ignore ACR=b)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_10_acr <- collect(sql(\"SELECT ACR,VALP FROM housing WHERE ACR=2 ORDER BY VALP\"))\n",
      "nrow(housing_valp_10_acr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_10_acr_median = (housing_valp_10_acr[nrow(housing_valp_10_acr)/2,\"VALP\"] + housing_valp_10_acr[nrow(housing_valp_10_acr)/2+1,\"VALP\"] ) / 2.0\n",
      "housing_valp_10_acr_median"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last one."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_10more_acr <- collect(sql(\"SELECT ACR,VALP FROM housing WHERE ACR=3 ORDER BY VALP\"))\n",
      "nrow(housing_valp_10more_acr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_10more_acr_median = (housing_valp_10more_acr[nrow(housing_valp_10more_acr)/2,\"VALP\"] + housing_valp_10more_acr[nrow(housing_valp_10more_acr)/2+1,\"VALP\"] ) / 2.0\n",
      "housing_valp_10more_acr_median"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So regarding median values, it seems to be a direct relationship between lot size and property value, although we don't know how meaningful it might be.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Standard deviation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the mean values we already got, and calculate the standard deviation as follows. First we use `sql` to select and filter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_valp_1_sql <- sql(\"SELECT VALP FROM housing WHERE ACR=1\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That gives us a `DataFrame` with all the `VALP` values for `ACR=1`. Next, we can add a new column with the squared differences between these `VALP` values and the average `VALP` value for `ACR=1` we calculated before by grouping and aggregations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_1_sqr_diff <- transform(\n",
      "    housing_valp_1_sql, \n",
      "    SQR_DIFF = (housing_valp_1_sql$VALP - housing_valp_by_acr[2,\"AVG_VALP\"])**2\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we use `agg` to sum the squared differences and collect the result (will be a single row R data frame)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_1_sum_sqr_diff <- collect(agg(\n",
      "    housing_valp_1_sqr_diff,\n",
      "    SUM_SQR_DIFF = sum(housing_valp_1_sqr_diff$SQR_DIFF),\n",
      "    N = count(housing_valp_1_sqr_diff$SQR_DIFF)\n",
      "))\n",
      "housing_valp_1_sum_sqr_diff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally we divide by N and get the square root and we have the standard deviation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_1_sd <- sqrt(housing_valp_1_sum_sqr_diff[1,\"SUM_SQR_DIFF\"] / housing_valp_1_sum_sqr_diff[1,\"N\"])\n",
      "housing_valp_1_sd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's for `ACR=1`, let's do the same for the other `ACR` values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_10_sql <- sql(\"SELECT VALP FROM housing WHERE ACR=2\")\n",
      "housing_valp_10_sqr_diff <- transform(\n",
      "    housing_valp_10_sql, \n",
      "    SQR_DIFF = (housing_valp_10_sql$VALP - housing_valp_by_acr[3,\"AVG_VALP\"])**2\n",
      ")\n",
      "housing_valp_10_sum_sqr_diff <- collect(agg(\n",
      "    housing_valp_10_sqr_diff,\n",
      "    SUM_SQR_DIFF = sum(housing_valp_10_sqr_diff$SQR_DIFF),\n",
      "    N = count(housing_valp_10_sqr_diff$SQR_DIFF)\n",
      "))\n",
      "housing_valp_10_sd <- sqrt(housing_valp_10_sum_sqr_diff[1,\"SUM_SQR_DIFF\"] / housing_valp_10_sum_sqr_diff[1,\"N\"])\n",
      "housing_valp_10_sd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last one."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_10m_sql <- sql(\"SELECT VALP FROM housing WHERE ACR=3\")\n",
      "housing_valp_10m_sqr_diff <- transform(\n",
      "    housing_valp_10m_sql, \n",
      "    SQR_DIFF = (housing_valp_10m_sql$VALP - housing_valp_by_acr[4,\"AVG_VALP\"])**2\n",
      ")\n",
      "housing_valp_10m_sum_sqr_diff <- collect(agg(\n",
      "    housing_valp_10m_sqr_diff,\n",
      "    SUM_SQR_DIFF = sum(housing_valp_10m_sqr_diff$SQR_DIFF),\n",
      "    N = count(housing_valp_10m_sqr_diff$SQR_DIFF)\n",
      "))\n",
      "housing_valp_10m_sd <- sqrt(housing_valp_10m_sum_sqr_diff[1,\"SUM_SQR_DIFF\"] / housing_valp_10m_sum_sqr_diff[1,\"N\"])\n",
      "housing_valp_10m_sd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We don't really need to go much further. Considering these huge standard deviation values and how close the average values are, those three populations are totally overlapped, and using `ACR` as a predictor for `VALP` probably won't make much sense since it is difficult to differentiate between the three classes. In any case we will find this out when building our linear model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exploring electricity monthly cost"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Electricity (monthly cost) can take the following values:  \n",
      "- bbb: N/A (GQ/vacant)  \n",
      "- 001: Included in rent or in condo fee  \n",
      "- 002: No charge or electricity not used  \n",
      "- 003..999: 3 to 999 USD (Rounded and top-coded)  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have a quick look by using `describe`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collect(describe(select(housing_df, \"ELEP\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We tried the not so scalable approach of collecting all the data and using ggplot2 with all of it, bit got all sort of errors while doing it. We we can do here is to sample down the million and a half samples into something smaller. SparkR provides a function called `sample` for that purpose. We can use it as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_df_sample <- sample(housing_df, withReplacement=FALSE, fraction=0.01, seed=1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we can select the columns we need as follows and collect into an R `data.frame` we will pass to ggplot2."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "housing_valp_elep <- collect(select(housing_df_sample, \"VALP\", \"ELEP\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "str(housing_valp_elep)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we can create the ggplot object and use `geom_point` to display a scatter plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "elep_chart <- ggplot(data=housing_valp_elep, aes(x=ELEP, y=VALP))\n",
      "elep_chart + geom_point() + ylim(0,2e+06) + xlim(3,600) + geom_smooth()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not that we can conclude a lot from there. We added a [`geom_smmoth`](http://docs.ggplot2.org/0.9.3.1/geom_smooth.html) so we can better see any relationship between the two variables. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's say we want a third variable into place. For example, let's colour our points using the `ACR` variable we explored before, in order to check if we can find out something in a scatter plot. We need to re-select our data using SparkR in order to include the variable `ACR` as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_elep_acr <- collect(select(housing_df_sample, \"VALP\", \"ELEP\",\"ACR\"))\n",
      "str(housing_valp_elep_acr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can plot again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "elep_acr_chart <- ggplot(data=housing_valp_elep_acr, aes(x=ELEP, y=VALP,colour=as.factor(ACR)))\n",
      "elep_acr_chart + geom_point() + ylim(0,2e+06) + xlim(3,600) + geom_smooth()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It seems that, although we couldn't conclude a clear relationship between `ACR` and propery value as a whole, when we putting together `ACR` and `ELEP` there is a more clear relationship between `ELEP` and `VALP` for some values of `ACR` (i.e. the geom_smoot is smoother)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's give this idea a try. Let's filter our dataset by `ACR` and repetat the previous visualisation for individual `ACR` values. We could filter the R data frame we already have, but let's using SparkR filtering capabilities so we have a bigger sample."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "housing_valp_elep_acr_1 <- filter(select(housing_df, \"VALP\", \"ELEP\",\"ACR\"), \"ACR=1\")\n",
      "housing_valp_elep_acr_1_sample <- collect(sample(\n",
      "    housing_valp_elep_acr_1, \n",
      "    withReplacement=FALSE, fraction=0.05, seed=1234\n",
      "))\n",
      "str(housing_valp_elep_acr_1_sample)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "elep_acr_1to10_chart <- ggplot(data=housing_valp_elep_acr_1_sample, aes(x=ELEP, y=VALP))\n",
      "elep_acr_1to10_chart + geom_point() + ylim(0,2e+06) + xlim(3,600) + geom_smooth()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "collapsed": true
     },
     "source": [
      "Closing thoughts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is still a lot missing from SparkR in order to be able to work with it as we do with regular R, mainly in the area of descriptive statistics. It would be great to have methods to calculate standard deviations of columns, or covariance and correlations between them. Also different quartiles, IQR, etc. I hope they will be available in firther versions of the library."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apart from that is great to be able to use a Spark cluster to process data, specially when doing grouping and aggregations, or being able to use SQL to select and filter our datasets."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}